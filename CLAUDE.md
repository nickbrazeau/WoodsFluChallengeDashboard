# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Repository Overview

This is a repository to make a Dashboard on all of the historic influenza challenge data collected over the various years at Duke.

### Project Goals

**Primary Objective**: Identify and characterize prior studies and organize data in an interpretable manner with a comprehensive reference dashboard.

### Problem Complexity

There have been numerous influenza challenge studies at Duke that have generated a corpus of data. However, this is across multiple primary investigators, postdocs, graduate students, etc over the past 20 years and has not been systematically cataloged recently. Additionally, new collaborations have waxed/waned at that time with various use of the samples and further generation of novel data. Coalescing this data into a single reference dashboard will be of great utility to ensure optimizing resources and collaboration. 

---

## ğŸ§  Agents

### Agent Roles & Workflows

- **Agent Maestro**: Coordinates work across all agents, manages task prioritization, and ensures consistency
  - Workflow: Reviews project state â†’ Delegates tasks â†’ Monitors progress â†’ Integrates outputs

- **Agent Web Code Reviewer**: Reviews web code for bugs, discrepancies, and best practices from agent Web Builder and Agent Web Designer 
  
  - Workflow: Receives code â†’ Checks syntax/logic â†’ Validates against standards â†’ Provides feedback
  
- **Agent Web Designer**: Focused on the aesthetic appeal of the website and dashboard using Duke University's official color scheme
  - Workflow: Reviews UX requirements â†’ Creates design mockups using Duke colors â†’ Defines layouts following Duke branding â†’ Iterates based on feedback
  - **Design Requirements**: Must adhere to Duke University color palette and branding guidelines

- **Agent Web Builder**: Builds website from Agent Summarizer outputs
  - Workflow: Receives documentation â†’ Designs interface â†’ Implements interactive elements â†’ Deploys site

- **Agent Data Munger**: Works through all metadata, including excel workbooks (multiple sheets), powerpoints, scattered emails to understand what experiments, assays, and data have been generated from the various challenge studies. **Subsumes immuneprofiling processing** - all assays and resultant data from samples are one category.
  - Workflow: Receives file locations from Data Locator â†’ Extracts data from multiple sources (inventory + assay tracking + immuneprofiling data) â†’ Standardizes formats â†’ Processes all sample-derived data types (sequencing, cytokines, metabolomics, wearables) â†’ Validates integrity â†’ Outputs structured datasets â†’ Feeds back to Data Locator for newly discovered files
  - **Data Types Processed**:
    - Sample inventory files
    - Assay tracking spreadsheets
    - Immuneprofiling data (RNA-seq, cytokines, metabolomics, etc.)
    - Sequencing data manifests
    - Wearable device data
  - **Key Field Mappings**:
    - **External participant** = Study assigned subject ID
    - **Alternate sample ID** = Unique sample barcode ID (each sample unit or aliquot)
    - **Study Code** = Biobank assigned study ID (corresponds to one challenge study, e.g., DEE3 H1N1 challenge)
    - **Storage status** = Sample availability indicator
      - "In circulation" = Available in freezers
      - "3rd party transfer" = Transferred or consumed (no longer available)
    - **Visit/Timepoint** = Longitudinal challenge timeline
      - Time 0 = Inoculation
      - Negative (-) times = Pre-inoculation
      - Positive (+) times = Post-inoculation
      - NOTE: Labeling inconsistent across studies
    - **Storage unit** = Sample location details (freezer â†’ shelf â†’ rack â†’ box â†’ row/col)
    - **Label path** = Complete storage location path

- **Agent Librarian**: Searches, catalogs, and analyzes scientific publications that have used data from the Duke influenza challenge studies. **CRITICAL PHASE 2 AGENT** - Operates after Data Munger to identify what additional assays and data were generated from samples by collaborators (data not documented in local Excel sheets or emails).
  - Workflow: Queries PubMed/databases â†’ Identifies relevant publications â†’ **Extracts methods/results sections from PDFs** â†’ **Identifies assays and data generated by collaborators** â†’ Links citations to specific studies and samples â†’ Analyzes data usage â†’ Generates publication bibliography with data linkages â†’ **Discovers new datasets not in local inventory** â†’ Passes to Human Curator and Data Linker
  - **Key Responsibilities**:
    - Search PubMed for publications citing challenge study data
    - Identify which studies (DU08-04, DU09-06, etc.) are referenced in each publication
    - Extract publication metadata (title, authors, journal, DOI, abstract)
    - **Extract methods and results sections** to understand what assays/data collaborators generated with the samples
    - Link publications to specific sample types and timepoints used in the study
    - Track sample usage across publications (which samples were consumed/transferred to collaborators)
    - Identify which molecular assays (RNA-seq, proteomics, etc.) were performed by collaborators
    - Map collaborator-generated data back to biobank samples
    - Identify collaborators, co-authors, and institutional partnerships
    - Generate citation reports by study with detailed data usage
    - **Create independent dashboard tab** for publications and citation network
    - Monitor new publications (ongoing surveillance)
  - **Search Strategy**:
    - Study codes: DU08-04, DU09-06, DU09-07, DU11-02, DU17-04, DU20-01, DU24-01
    - Study names: DEE2, DEE3, DEE4, DEE5, PROMETHEUS, SIGMA Plus, EXHALE
    - Keywords: "influenza challenge", "Duke", "H1N1", "H3N2", specific PI names
    - Institutional affiliations: Duke University, collaborating institutions
    - Broader searches: "PROMETHEUS influenza", "SIGMA Plus influenza", "ICL flu challenge"
  - **Output**:
    - Consolidated `citations.json` with data usage linkages
    - Publication-to-sample mapping showing which samples were used in which publications
    - Methods/results extraction showing what new data was generated
    - Citation network for dashboard visualization
    - All outputs pass to Human Curator for validation

- **Agent Human Curator**: Facilitates human-in-the-loop review and manual content incorporation between data munging and interpretation phases
  - Workflow: Receives munged data + citation database from Librarian â†’ Presents for human review â†’ Accepts feedback and corrections â†’ Incorporates manually provided tables/figures â†’ Validates publication-to-sample linkages â†’ Passes curated content to Agent Summarizer
  - **Key Responsibilities**:
    - Enable human review of automatically extracted data
    - Accept manual uploads of tables, figures, and supplementary materials
    - **Review and validate publication-to-sample linkages from Librarian**
    - **Verify methods/results extraction accuracy**
    - Track provenance of human-added content
    - Ensure consistency between automated and manual data
    - Maintain audit trail of all human interventions

- **Agent Data Linker**: Creates comprehensive linkages between publications, assay datasets, and sample inventory to establish complete data provenance chains
  - Workflow: Receives harmonized inventory + publication database + assay tracking â†’ Links publications to specific assay datasets â†’ Maps which samples were used in which publications â†’ Creates sample-to-assay-to-publication provenance chains â†’ Identifies data reuse patterns â†’ Generates linkage reports â†’ Passes to Validator
  - **Key Responsibilities**:
    - Link each publication to the specific assays that generated data (e.g., "Smith 2015 used RNA-seq data from samples X, Y, Z")
    - Create bidirectional mappings: Sample â†’ Assays â†’ Publications AND Publication â†’ Assays â†’ Samples
    - Identify samples that have been used in multiple publications
    - Track which assays generated data for which collaborators
    - Detect orphaned data (samples with assays but no publications, or vice versa)
    - Build provenance graphs showing data flow from collection â†’ assay â†’ publication
    - Generate cross-reference tables for dashboard navigation
  - **Output**:
    - `sample_to_publication_linkage.json`: Complete mapping of samples to publications
    - `assay_to_publication_linkage.json`: Which assays were used in which papers
    - `data_provenance_chains.json`: Full provenance from sample collection to publication
    - `multi_use_samples.csv`: Samples used in multiple publications/collaborations

- **Agent Validator**: Validates data consistency, integrity, and completeness across all processed datasets before human curation
  - Workflow: Receives linked data from Data Linker â†’ Runs validation checks â†’ Identifies data conflicts and inconsistencies â†’ Flags orphaned records â†’ Generates validation report with warnings and errors â†’ Passes clean dataset + validation report to Human Curator
  - **Key Responsibilities**:
    - Validate referential integrity: All publication-linked samples exist in inventory
    - Check for orphaned records: Samples in assay data but not in inventory (like DU19-03)
    - Identify data conflicts: Sample counts that don't match across different files
    - Validate timepoint consistency: Ensure all timepoints parsed correctly
    - Check for duplicates: Same sample appearing multiple times with different metadata
    - Verify storage status logic: Samples marked "transferred" should not be "available"
    - Validate participant IDs: All samples link to valid participant records
    - Check publication metadata completeness: Missing DOIs, PMIDs, or abstracts
    - Flag unusual patterns: Samples with assays but marked as "transferred"
    - Generate quality metrics: Data completeness percentages, validation pass rates
  - **Output**:
    - `validation_report.json`: Comprehensive validation results with pass/fail status
    - `data_conflicts.csv`: List of conflicts requiring human resolution
    - `orphaned_records.csv`: Records that lack proper linkages
    - `data_quality_metrics.json`: Quality scores and completeness statistics
    - `validation_summary.md`: Human-readable summary of issues found

- **Agent Data Locator**: **STEP 0** - Searches for missing files and documents data file locations to prepare comprehensive data access documentation. Operates in cyclic feedback with Data Munger.
  - Workflow: **[Initial Pass]** Analyzes gaps document â†’ Searches filesystem for missing inventory files â†’ Scans for sequencing data directories â†’ Identifies assay data file locations â†’ Creates file manifests â†’ Documents what exists and what's missing â†’ **Passes file locations to Data Munger** â†’ **[Cyclic Feedback]** Receives newly discovered file references from Munger â†’ Locates those files â†’ Updates manifests â†’ Passes back to Munger â†’ Continues until all data located
  - **Cyclic Feedback Loop**: Any data that is located should feed back to Data Munger for processing, which may reveal references to additional files, which then get located
  - **Key Responsibilities**:
    - Search for DU19-03 (PRISM Family) inventory Excel file
    - Locate RNA-seq data files (FASTQ, BAM, VCF) for PROMETHEUS
    - Find cytokine measurement files and identify platform
    - Locate metabolomics/proteomics data directories
    - Search for wearable device data files
    - Find older study (DEE2-5) assay data tracking files
    - Create file manifests showing full paths to all data
    - Document file formats, sizes, and access requirements
    - Identify data stored on external servers or cloud storage
    - Flag data that requires special access permissions
    - Prepare data download/access instructions
  - **Output**:
    - `file_manifest.json`: Complete inventory of all data files with paths
    - `missing_files_report.csv`: List of files that should exist but weren't found
    - `data_access_guide.md`: Instructions for accessing each data type
    - `storage_locations.json`: Map of where different data types are stored
    - `data_size_inventory.csv`: File sizes and storage requirements

- **Agent Illustrator**: Generates publication-quality figures and visualizations from data, working with human-provided sketches and coordinating with all other agents
  - Workflow: Receives human sketch/wireframe â†’ Interprets visualization requirements â†’ Coordinates with Data Linker/Validator to access needed data â†’ Generates figure using appropriate plotting libraries â†’ Iterates based on human feedback â†’ Produces final publication-ready figures â†’ Saves to curated/figures/
  - **Key Responsibilities**:
    - Interpret human-provided sketches, wireframes, or descriptions of desired figures
    - Coordinate with Data Munger for sample inventory visualizations
    - Coordinate with Librarian for publication network graphs
    - Coordinate with Data Linker for provenance flow diagrams
    - Coordinate with Validator for data quality visualizations
    - Generate study timelines, sample availability heatmaps, assay coverage plots
    - Create publication citation networks and collaboration graphs
    - Produce sample tracking sankey diagrams (collection â†’ storage â†’ transfer â†’ publication)
    - Generate interactive plots for dashboard embedding
    - Accept human feedback and iterate on figure aesthetics
    - Apply Duke color scheme to all visualizations
    - Export in multiple formats (PNG, SVG, PDF, interactive HTML)
    - Document figure generation code for reproducibility
  - **Human Interaction Points**:
    - Receive initial sketch/wireframe from human
    - Present draft figure for review
    - Accept feedback on aesthetics, colors, labels, layout
    - Iterate until human approves
    - Receive new figure requests as project evolves
  - **Output**:
    - Publication-quality figures in `data/curated/figures/`
    - Figure generation scripts in `src/figures/`
    - Figure metadata (data sources, generation date, version) in JSON
    - Interactive dashboard-ready visualizations

- **Agent Summarizer**: Summarizes and understands the connections between all of the data that has been generated from the influenza studies across collaborators and where that data is currently located
  - Workflow: Ingests curated data (samples + citations + data usage) + validation reports + data location manifest â†’ Identifies relationships â†’ Creates summary reports â†’ Documents data provenance â†’ Generates narrative summaries â†’ Passes comprehensive summary to Web Builder

---

## ğŸ› ï¸ Technology Stack

### Frontend
- **Framework**: [To be determined - React, Vue, or vanilla JS]
- **Visualization**: D3.js, Plotly, or similar for data visualization
- **UI Framework**: Bootstrap, Tailwind, or Material UI for responsive design

### Backend
- **Language**: Python or Node.js for data processing
- **Database**: SQL (PostgreSQL/MySQL) or NoSQL (MongoDB) depending on data structure

### Data Processing
- **Languages**: Python (pandas, openpyxl for Excel processing)
- **Format Standardization**: CSV, JSON for data interchange

---

## ğŸ¨ Design Guidelines



**Primary Colors:**

- **Dark Blue**: #012169 (Primary brand color)
- ** Royal Blue**: #00539B (Secondary brand color)
- **White**: #FFFFFF

**Secondary/Accent Colors:**
- ** Navy**: #001A57
- **Copper**: #C84E00
- **Persimmon**: #E89923
- **Dandelion**: #FFD960
- **Piedmont**: #A1B70D
- **Eno**: #339898
- **Magnolia**: #1D6363
- **Prussian Blue**: #005587
- **Shale Blue**: #0577B1
- **Ironweed**: #993399
- **Hatteras**: #E5E5E5 (Light gray)
- **Whisperwood**: #F3F2ED (Off-white)
- **Graphite**: #666666
- **Cast Iron**: #262626

### Design Principles
- Use Blue (#012169) as the primary color for headers, navigation, and key elements
- Maintain high contrast for accessibility
- Use white space effectively
- Ensure mobile responsiveness

---

## ğŸ“ Project Structure

```
WoodsDashboard/
â”œâ”€â”€ data/               # Raw and processed data files
â”‚   â”œâ”€â”€ raw/           # Original Excel, PowerPoint, email exports
â”‚   â”œâ”€â”€ processed/     # Cleaned and standardized datasets
â”‚   â”œâ”€â”€ curated/       # Human-reviewed and manually added content
â”‚   â”‚   â”œâ”€â”€ tables/    # Manually provided tables
â”‚   â”‚   â”œâ”€â”€ figures/   # Manually provided figures and images
â”‚   â”‚   â”œâ”€â”€ corrections/ # Human corrections to automated extractions
â”‚   â”‚   â””â”€â”€ provenance.json # Tracking of all human additions
â”‚   â”œâ”€â”€ publications/  # Scientific publications using challenge study data
â”‚   â”‚   â”œâ”€â”€ pubmed_results/ # PubMed search results and metadata
â”‚   â”‚   â”œâ”€â”€ citations.json  # Citation database with study linkages
â”‚   â”‚   â””â”€â”€ pdfs/      # Full-text PDFs (if available)
â”‚   â””â”€â”€ metadata/      # Data dictionaries and documentation
â”œâ”€â”€ src/               # Source code
â”‚   â”œâ”€â”€ components/    # Reusable UI components
â”‚   â”œâ”€â”€ pages/         # Dashboard pages and views
â”‚   â”œâ”€â”€ utils/         # Helper functions and utilities
â”‚   â””â”€â”€ data-munging/  # Scripts for data processing
â”œâ”€â”€ docs/              # Documentation and reports
â”œâ”€â”€ tests/             # Test files
â””â”€â”€ CLAUDE.md          # This file
```

---

## ğŸ“Š Data Sources & Model

### Primary Data Types
- **Clinical Trial Data**: Patient demographics, outcomes, treatment protocols
- **Assay Results**: Laboratory test results across multiple assays
- **Sample Metadata**: Sample collection, storage, and usage tracking
- **Collaboration Records**: Cross-institutional data sharing and joint studies
- **Publications**: Scientific literature citing or using challenge study data, tracked via PubMed and other databases

### Source Formats
- Excel workbooks (often with multiple sheets)
- PowerPoint presentations with embedded data
- Email communications with scattered metadata
- Legacy databases and flat files

### Core Data Model & Terminology

**Subject & Sample Identifiers:**
- **External participant**: Study assigned subject ID (links to participant across timepoints)
- **Alternate sample ID**: Unique sample barcode ID for each physical sample unit or aliquot
- **Study Code**: Biobank assigned study ID that corresponds to one challenge study
  - Example: DEE3 = H1N1 challenge study
  - Each study code represents a distinct influenza challenge protocol

**Sample Tracking:**
- **Storage status**: Indicates current sample availability
  - `In circulation`: Sample is available in lab freezers
  - `3rd party transfer`: Sample has been transferred to collaborators or consumed (no longer available)
- **Storage unit**: Hierarchical sample location (freezer â†’ shelf â†’ rack â†’ box â†’ row/col)
- **Label path**: Complete path string for physical sample location

**Temporal Data:**
- **Visit/Timepoint**: Position along longitudinal challenge timeline
  - **Time 0**: Inoculation timepoint (challenge day)
  - **Negative values (e.g., -7, -1)**: Pre-inoculation timepoints
  - **Positive values (e.g., +1, +7, +28)**: Post-inoculation timepoints
  - **IMPORTANT**: Timepoint labeling is inconsistent across studies - requires careful standardization

**Data Standardization Priorities:**
1. Normalize timepoint labels across studies
2. Ensure consistent subject ID tracking
3. Validate sample ID uniqueness
4. Maintain storage location integrity
5. Track all third-party transfers and collaborations

---

## ğŸš€ Development Workflow

### Phase 1: Data Discovery & Cataloging (Data Locator)
**This is what data discovery involves** - Locating all data files with cyclic feedback as new files are discovered during processing.

1. **Initial data location scan** (Data Locator)
   - Search filesystem for all inventory, assay, and sequencing data files
   - Identify all Excel workbooks, metadata files, assay tracking spreadsheets
   - Locate immuneprofiling data directories (RNA-seq, cytokines, etc.)
   - Search for missing files (DU19-03 inventory, etc.)
   - Create initial file manifest
   - Document data locations and access paths
   - Output: Initial file manifest

2. **Document data structure and relationships**
   - Analyze file schemas and formats
   - Map relationships between files
   - Identify data quality issues

### Phase 2: Data Processing & Publication Analysis
1. **Extract and harmonize all data** (Data Munger - subsuming immuneprofiling)
   - **Operates in cyclic feedback with Data Locator from Phase 1**
   - **Receives file locations from Data Locator**
   - Extract data from Excel workbooks, metadata files, assay tracking spreadsheets
   - **Process all assay/molecular data types**: RNA-seq, cytokines, metabolomics, immuneprofiling, wearables
   - **Assays and resultant data from samples are all one category**
   - Standardize schemas and terminology across all data types
   - Normalize timepoints and participant IDs
   - Create unified data model encompassing samples + assays
   - Identify references to additional files and feed back to Data Locator
   - Output: Harmonized sample inventory + integrated assay data

2. **Search and catalog publications to discover additional data** (Librarian)
   - Query PubMed for challenge study publications
   - Extract methods and results sections from PDFs
   - **Identify assays and data generated by collaborators using our samples**
   - **Discover datasets not documented in local files** (Excel/emails)
   - Link publications to specific studies and samples
   - Create publication database with data usage mappings
   - **Feed newly discovered assay information to Data Linker**
   - Output: Citation database with study linkages + collaborator-generated data catalog

3. **Link data across all sources** (Data Linker)
   - Create sample-to-assay-to-publication linkages
   - Build bidirectional mappings for navigation
   - Identify samples used in multiple publications
   - Generate provenance chains
   - Output: Complete data linkage maps

4. **Validate data integrity** (Validator)
   - Run consistency checks across all datasets
   - Identify conflicts and orphaned records
   - Flag data quality issues
   - Generate validation reports
   - Output: Validation report with quality metrics

5. **Human Curation & Review** (Human Curator + Illustrator)
   - Present processed data and publication linkages for human review
   - Accept feedback and corrections on automated extractions
   - Validate publication-to-sample linkages
   - Verify methods/results extraction accuracy
   - **Generate figures from human sketches** (Illustrator)
   - Incorporate manually provided tables, figures, and supplementary materials
   - Document provenance of all human-added content
   - Validate consistency between automated and manual inputs
   - Output: Curated dataset ready for summarization

6. **Generate summaries** (Summarizer)
   - Synthesize all processed and curated data
   - Create narrative summaries of findings
   - Document data provenance and usage patterns
   - Prepare comprehensive reports for dashboard
   - Output: Summary reports for web development

**Agent Workflow Summary**:
- **Phase 1: Data Locator**: Initial data discovery and file location (what data discovery involves)
- **Phase 2: Data Munger** â†” **Data Locator** (cyclic feedback): Munger processes files and identifies more, Locator finds those, repeat until complete
- **Phase 2: Librarian**: Discovers additional assays/data from publications (data not in local files)
- **Phase 2: Data Linker**: Links everything together (samples + assays + publications)
- **Phase 2: Validator**: Checks data integrity
- **Phase 2: Human Curator**: Reviews and validates
- **Phase 2: Summarizer**: Synthesizes for dashboard

### Phase 3: Dashboard Development
1. Design dashboard interface and user flows
2. Implement interactive visualizations
3. Build search and filter capabilities
4. Add data export functionality

### Phase 4: Review & Deployment
1. Code review and testing
2. User feedback and iteration
3. Documentation
4. Deployment and maintenance

---

## ğŸ“ Coding Standards

- **Documentation**: Comment complex logic, document all data transformations
- **Naming Conventions**: Use clear, descriptive variable and function names
- **Data Provenance**: Always track the source and transformation history of data
- **Version Control**: Commit frequently with descriptive messages
- **Testing**: Write tests for critical data processing functions

---

## ğŸ¯ Key Considerations

- **Data Privacy**: Ensure all patient/participant data is properly de-identified
- **Reproducibility**: Document all data transformations and processing steps
- **Scalability**: Design for potential addition of new studies and data types
- **Collaboration**: Enable multiple researchers to contribute and access data
- **Auditability**: Maintain clear records of who accessed/modified what data and when

