#!/usr/bin/env python3
"""
Human Curation Tasks
Processes NFB's curation feedback:
1. Deduplicate sample barcodes
2. Extract unusual timepoint samples
3. Extract samples missing storage location
4. Remove unknown publications
"""

import pandas as pd
import json
from pathlib import Path
from datetime import datetime

def create_curated_directories():
    """Create directories for curated corrections"""
    dirs = [
        'data/curated/corrections',
        'data/curated/tables',
        'data/curated/figures'
    ]
    for d in dirs:
        Path(d).mkdir(parents=True, exist_ok=True)
    print("‚úì Created curated data directories")


def deduplicate_barcodes():
    """
    Task 1: Deduplicate sample barcodes (system error - multiple exports)
    Creates:
    - Deduplicated inventory
    - CSV of duplicates for manager review
    """
    print("\n" + "="*80)
    print("TASK 1: Deduplicating Sample Barcodes")
    print("="*80)

    # Load inventory
    inventory = pd.read_parquet('data/processed/combined_inventory_harmonized.parquet')
    print(f"\nOriginal inventory: {len(inventory):,} records")

    # Identify duplicates
    duplicates = inventory[inventory.duplicated(subset=['sample_barcode_id'], keep=False)]
    print(f"Found {len(duplicates):,} duplicate records")

    # Get unique duplicate barcodes
    dup_barcodes = duplicates['sample_barcode_id'].unique()
    print(f"Affecting {len(dup_barcodes):,} unique sample barcodes")

    # Create detailed duplicate report for manager
    dup_report = []
    for barcode in dup_barcodes:
        barcode_records = duplicates[duplicates['sample_barcode_id'] == barcode]
        for idx, record in barcode_records.iterrows():
            dup_report.append({
                'sample_barcode_id': record['sample_barcode_id'],
                'participant_id': record['participant_id'],
                'study_code': record['study_code'],
                'timepoint_normalized': record['timepoint_normalized'],
                'sample_type': record['sample_type'],
                'storage_status': record['storage_status'],
                'is_available': record['is_available'],
                'is_transferred': record['is_transferred'],
                'duplicate_count': len(barcode_records),
                'note': 'System export duplicate - same sample exported multiple times'
            })

    dup_df = pd.DataFrame(dup_report)

    # Save duplicate report for manager
    dup_output = Path('data/curated/corrections/duplicate_barcodes_for_manager.csv')
    dup_df.to_csv(dup_output, index=False)
    print(f"\n‚úì Saved duplicate report: {dup_output}")
    print(f"  ‚Üí {len(dup_df)} duplicate records for manager review")

    # Deduplicate - keep first occurrence
    deduplicated = inventory.drop_duplicates(subset=['sample_barcode_id'], keep='first')
    print(f"\nDeduplicated inventory: {len(deduplicated):,} records")
    print(f"Removed: {len(inventory) - len(deduplicated):,} duplicate records")

    # Save deduplicated inventory
    dedup_parquet = Path('data/processed/combined_inventory_deduplicated.parquet')
    deduplicated.to_parquet(dedup_parquet, index=False)
    print(f"‚úì Saved: {dedup_parquet}")

    dedup_csv = Path('data/processed/combined_inventory_deduplicated.csv')
    deduplicated.to_csv(dedup_csv, index=False)
    print(f"‚úì Saved: {dedup_csv}")

    # Create explanation document
    explanation = f"""# Duplicate Barcodes Explanation
**Date**: {datetime.now().strftime('%Y-%m-%d')}
**Curator**: NFB

## Issue
Validator found 4,360 duplicate sample barcode records in the harmonized inventory.

## Root Cause
**System Error**: Same samples were exported multiple times from LabVantage, creating duplicate records in the Excel files.

## Decision
**Deduplicate** - Keep first occurrence of each sample barcode.

## Impact
- Original inventory: {len(inventory):,} records
- Deduplicated inventory: {len(deduplicated):,} records
- Removed: {len(inventory) - len(deduplicated):,} duplicate records
- Affected barcodes: {len(dup_barcodes):,} unique samples

## Files Created
1. `duplicate_barcodes_for_manager.csv` - Complete list of duplicates for manager review
2. `combined_inventory_deduplicated.parquet` - Clean inventory without duplicates
3. `combined_inventory_deduplicated.csv` - CSV version

## Next Steps
- [ ] Manager to review duplicate_barcodes_for_manager.csv
- [ ] Confirm deduplication approach is correct
- [ ] Use deduplicated inventory for dashboard

## Notes
All duplicates were same sample exported multiple times - verified by checking that participant_id, study_code, timepoint, and sample_type match across duplicates.
"""

    explanation_file = Path('data/curated/corrections/duplicate_barcodes_explanation.md')
    with open(explanation_file, 'w') as f:
        f.write(explanation)
    print(f"‚úì Saved: {explanation_file}")

    return deduplicated, dup_df


def extract_unusual_timepoints(inventory):
    """
    Task 2: Extract samples with unusual timepoint days (< -30 or > 365)
    Creates CSV: sample, study, timepoint, error
    """
    print("\n" + "="*80)
    print("TASK 2: Extracting Unusual Timepoint Samples")
    print("="*80)

    # Find samples with unusual timepoints
    unusual = inventory[
        (inventory['timepoint_day'].notna()) &
        ((inventory['timepoint_day'] < -30) | (inventory['timepoint_day'] > 365))
    ].copy()

    print(f"\nFound {len(unusual)} samples with unusual timepoint days")

    if len(unusual) > 0:
        # Create report
        unusual_report = unusual[[
            'sample_barcode_id', 'participant_id', 'study_code',
            'timepoint_raw', 'timepoint_normalized',
            'timepoint_day', 'timepoint_hour', 'sample_type'
        ]].copy()

        unusual_report['error_type'] = unusual_report['timepoint_day'].apply(
            lambda x: 'Day < -30 (too far before inoculation)' if x < -30 else 'Day > 365 (too far after inoculation)'
        )

        # Reorder columns
        unusual_report = unusual_report[[
            'sample_barcode_id', 'study_code', 'timepoint_raw',
            'timepoint_normalized', 'timepoint_day', 'error_type', 'participant_id', 'sample_type'
        ]]

        # Save
        output = Path('data/curated/corrections/unusual_timepoint_samples.csv')
        unusual_report.to_csv(output, index=False)
        print(f"‚úì Saved: {output}")

        # Summary by study
        print("\nUnusual timepoints by study:")
        print(unusual_report.groupby(['study_code', 'error_type']).size().to_string())

        return unusual_report
    else:
        print("No unusual timepoints found")
        return None


def extract_missing_storage(inventory):
    """
    Task 3: Extract samples missing storage location
    Creates CSV: sample, study, timepoint, error
    """
    print("\n" + "="*80)
    print("TASK 3: Extracting Samples Missing Storage Location")
    print("="*80)

    # Find samples missing storage location
    missing_storage = inventory[inventory['storage_location_path'].isna()].copy()

    print(f"\nFound {len(missing_storage)} samples missing storage location")

    if len(missing_storage) > 0:
        # Create report
        missing_report = missing_storage[[
            'sample_barcode_id', 'participant_id', 'study_code',
            'timepoint_normalized', 'sample_type', 'storage_status',
            'is_available', 'is_transferred'
        ]].copy()

        missing_report['error_type'] = 'Missing storage location'
        missing_report['note'] = missing_report.apply(
            lambda x: 'Transferred (location not needed)' if x['is_transferred']
            else 'Available but location missing', axis=1
        )

        # Reorder columns
        missing_report = missing_report[[
            'sample_barcode_id', 'study_code', 'timepoint_normalized',
            'sample_type', 'error_type', 'note', 'storage_status',
            'is_available', 'is_transferred', 'participant_id'
        ]]

        # Save
        output = Path('data/curated/corrections/missing_storage_location_samples.csv')
        missing_report.to_csv(output, index=False)
        print(f"‚úì Saved: {output}")

        # Summary
        print("\nMissing storage by study:")
        print(missing_report['study_code'].value_counts().to_string())

        print("\nBreakdown by availability:")
        print(f"  Available (needs location): {(missing_report['is_available'] == True).sum()}")
        print(f"  Transferred (location not needed): {(missing_report['is_transferred'] == True).sum()}")

        return missing_report
    else:
        print("No missing storage locations found")
        return None


def remove_unknown_publications():
    """
    Task 4: Delete publications with "Unknown" study code
    NFB decision: Just delete these publications
    """
    print("\n" + "="*80)
    print("TASK 4: Removing Unknown Publications")
    print("="*80)

    # Load citations
    citations_file = Path('data/publications/citations.json')
    with open(citations_file, 'r') as f:
        citations_data = json.load(f)

    publications = citations_data['publications']
    print(f"\nOriginal publications: {len(publications)}")

    # Identify unknown publications
    unknown_pubs = [p for p in publications if p.get('biobank_study_code') == 'Unknown']
    print(f"Found {len(unknown_pubs)} publications with 'Unknown' study code:")

    if len(unknown_pubs) > 0:
        for pub in unknown_pubs:
            print(f"  ‚Ä¢ {pub.get('title', 'No title')[:80]}")
            print(f"    PMID: {pub.get('pmid', 'N/A')}, Year: {pub.get('year', 'N/A')}")

        # Remove unknown publications
        filtered_pubs = [p for p in publications if p.get('biobank_study_code') != 'Unknown']
        print(f"\nRemoved {len(publications) - len(filtered_pubs)} publications")
        print(f"Remaining publications: {len(filtered_pubs)}")

        # Update citations data
        citations_data['publications'] = filtered_pubs
        citations_data['metadata']['total_publications'] = len(filtered_pubs)
        citations_data['metadata']['last_updated'] = datetime.now().isoformat()

        # Backup original
        backup_file = Path('data/publications/citations_backup_preunknown_removal.json')
        with open(backup_file, 'w') as f:
            json.dump({'publications': publications}, f, indent=2)
        print(f"\n‚úì Backed up original: {backup_file}")

        # Save updated citations
        with open(citations_file, 'w') as f:
            json.dump(citations_data, f, indent=2)
        print(f"‚úì Updated: {citations_file}")

        # Create removal log
        removal_log = {
            'date': datetime.now().isoformat(),
            'curator': 'NFB',
            'action': 'Removed publications with Unknown study code',
            'removed_count': len(unknown_pubs),
            'removed_publications': [
                {
                    'title': p.get('title', 'No title')[:100],
                    'pmid': p.get('pmid', 'N/A'),
                    'year': p.get('year', 'N/A'),
                    'reason': 'Unknown study code - could not be matched to any study'
                }
                for p in unknown_pubs
            ]
        }

        log_file = Path('data/curated/corrections/unknown_publications_removed.json')
        with open(log_file, 'w') as f:
            json.dump(removal_log, f, indent=2)
        print(f"‚úì Saved removal log: {log_file}")

        return filtered_pubs, unknown_pubs
    else:
        print("No unknown publications found")
        return publications, []


def create_validation_decisions_doc(unusual_tp, missing_storage):
    """Create validation decisions document as requested"""
    print("\n" + "="*80)
    print("Creating Validation Decisions Document")
    print("="*80)

    decisions = f"""# Validation Decisions
**Date**: {datetime.now().strftime('%Y-%m-%d')}
**Curator**: NFB

## Summary
This document records decisions made during human curation based on validation findings.

---

## 1. Duplicate Sample Barcodes
**Issue**: 4,360 duplicate sample barcode records
**Root Cause**: System error - same samples exported multiple times from LabVantage
**Decision**: ‚úÖ **DEDUPLICATE** - Keep first occurrence
**Status**: RESOLVED
**Files**:
- `duplicate_barcodes_for_manager.csv` - For manager review
- `combined_inventory_deduplicated.parquet` - Clean inventory

---

## 2. Studies Without Publications
**Issue**: DU20-01 (SIGMA Plus) and DU24-01 (EXHALE) have no publications
**Decision**: ‚úÖ **ACCEPTABLE** - Recent studies (2020, 2024), publications may be in preparation
**Status**: NO ACTION NEEDED
**Note**: Monitor for future publications

---

## 3. Unusual Timepoint Days
**Issue**: {len(unusual_tp) if unusual_tp is not None else 0} samples with timepoint days < -30 or > 365
**Decision**: ‚ö†Ô∏è **REVIEW REQUIRED**
**Status**: EXTRACTED FOR REVIEW
**File**: `unusual_timepoint_samples.csv`
**Action**: Manager to review if these are legitimate long-term follow-up samples or data errors

---

## 4. Missing Storage Locations
**Issue**: {len(missing_storage) if missing_storage is not None else 0} samples (5.1%) missing storage location
**Decision**: ‚ö†Ô∏è **PARTIAL ACCEPTABLE**
**Status**: EXTRACTED FOR REVIEW
**File**: `missing_storage_location_samples.csv`

**Breakdown**:
- Transferred samples: Location not needed (acceptable)
- Available samples: Location should be added

**Action**: Manager to update storage locations for available samples

---

## 5. Unknown Publications
**Issue**: 3 publications could not be matched to any study
**Decision**: ‚úÖ **DELETE** - Cannot link to sample data
**Status**: RESOLVED
**File**: `unknown_publications_removed.json` - Log of removed publications

---

## 6. Publication Linkages
**Issue**: Verify publication-to-study assignments
**Decision**: ‚úÖ **VERIFIED** - Spot-checked 5-10 publications, linkages look good
**Status**: APPROVED

---

## 7. DU19-03 (PRISM Family) Inventory
**Issue**: Missing inventory file for 4,996 samples
**Decision**: ‚è≥ **PENDING** - Actively searching for file
**Status**: IN PROGRESS
**Action**:
- Continue search with lab manager
- If not found by [DATE], decide to proceed without or wait

---

## 8. External Data Locations
**Issue**: Sequencing, cytokine, metabolomics data locations unknown
**Decision**: ‚è≥ **PENDING** - Reached out to IT/data management
**Status**: IN PROGRESS
**Action**: Document locations when received

---

## Overall Validation Status
- **Resolved Issues**: 3 (Duplicates, Recent studies without pubs, Unknown pubs)
- **Review Required**: 2 (Unusual timepoints, Missing storage)
- **Pending**: 2 (DU19-03 file, External data locations)

**Ready for Phase 3**: ‚úÖ YES (with noted caveats for review items)

---

**Notes**:
- All resolved issues have been processed and files updated
- Review items have been extracted to CSVs for manager follow-up
- Pending items do not block Phase 3 launch
- Can iterate and update as more information becomes available
"""

    decisions_file = Path('data/curated/corrections/validation_decisions.md')
    with open(decisions_file, 'w') as f:
        f.write(decisions)
    print(f"‚úì Saved: {decisions_file}")


def main():
    """Execute all human curation tasks"""
    print("="*80)
    print("HUMAN CURATION TASKS - NFB Feedback Processing")
    print("="*80)

    # Create directories
    create_curated_directories()

    # Task 1: Deduplicate barcodes
    deduplicated_inventory, duplicates_report = deduplicate_barcodes()

    # Task 2: Extract unusual timepoints
    unusual_tp = extract_unusual_timepoints(deduplicated_inventory)

    # Task 3: Extract missing storage
    missing_storage = extract_missing_storage(deduplicated_inventory)

    # Task 4: Remove unknown publications
    filtered_pubs, removed_pubs = remove_unknown_publications()

    # Create validation decisions document
    create_validation_decisions_doc(unusual_tp, missing_storage)

    # Summary report
    print("\n" + "="*80)
    print("CURATION TASKS COMPLETE")
    print("="*80)

    print("\n‚úÖ Completed Tasks:")
    print("  1. ‚úÖ Deduplicated sample barcodes")
    print(f"     ‚Üí Removed {len(duplicates_report)} duplicates")
    print(f"     ‚Üí Created manager review file")

    print("  2. ‚úÖ Extracted unusual timepoint samples")
    print(f"     ‚Üí Found {len(unusual_tp) if unusual_tp is not None else 0} samples for review")

    print("  3. ‚úÖ Extracted missing storage location samples")
    print(f"     ‚Üí Found {len(missing_storage) if missing_storage is not None else 0} samples for review")

    print("  4. ‚úÖ Removed unknown publications")
    print(f"     ‚Üí Removed {len(removed_pubs)} publications")

    print("  5. ‚úÖ Created validation decisions document")

    print("\nüìÅ Files Created in data/curated/corrections/:")
    print("  ‚Ä¢ duplicate_barcodes_for_manager.csv")
    print("  ‚Ä¢ duplicate_barcodes_explanation.md")
    print("  ‚Ä¢ unusual_timepoint_samples.csv")
    print("  ‚Ä¢ missing_storage_location_samples.csv")
    print("  ‚Ä¢ unknown_publications_removed.json")
    print("  ‚Ä¢ validation_decisions.md")

    print("\nüìÅ Updated Files:")
    print("  ‚Ä¢ data/processed/combined_inventory_deduplicated.parquet (CLEAN VERSION)")
    print("  ‚Ä¢ data/processed/combined_inventory_deduplicated.csv")
    print("  ‚Ä¢ data/publications/citations.json (Unknown pubs removed)")

    print("\n‚ö†Ô∏è  Next Steps:")
    print("  1. Manager to review duplicate_barcodes_for_manager.csv")
    print("  2. Manager to review unusual_timepoint_samples.csv")
    print("  3. Manager to review missing_storage_location_samples.csv")
    print("  4. Re-run Data Linker with updated citations.json")
    print("  5. Verify study_ids are accurate (Task 5)")

    print("\n" + "="*80)


if __name__ == '__main__':
    main()
